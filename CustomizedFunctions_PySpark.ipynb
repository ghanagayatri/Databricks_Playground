{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this Notebook we will learn how to write a customized function for PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in our initial dataset\n",
    "For this first section, we're going to be working with a set of Apache log files. These log files are made available by Databricks via the databricks-datasets directory. This is made available right at the root directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log_files = \"/databricks-datasets/sample_logs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from pyspark.sql import Row\n",
    "APACHE_ACCESS_LOG_PATTERN = '^(\\S+) (\\S+) (\\S+) \\[([\\w:/]+\\s[+\\-]\\d{4})\\] \"(\\S+) (\\S+) (\\S+)\" (\\d{3}) (\\d+)'\n",
    "\n",
    "# Returns a dictionary containing the parts of the Apache Access Log.\n",
    "def parse_apache_log_line(logline):\n",
    "    match = re.search(APACHE_ACCESS_LOG_PATTERN, logline)\n",
    "    if match is None:\n",
    "        # Optionally, you can change this to just ignore if each line of data is not critical.\n",
    "        # For this example, we want to ensure that the format is consistent.\n",
    "        raise Exception(\"Invalid logline: %s\" % logline)\n",
    "    return Row(\n",
    "        ipAddress    = match.group(1),\n",
    "        clientIdentd = match.group(2),\n",
    "        userId       = match.group(3),\n",
    "        dateTime     = match.group(4),\n",
    "        method       = match.group(5),\n",
    "        endpoint     = match.group(6),\n",
    "        protocol     = match.group(7),\n",
    "        responseCode = int(match.group(8)),\n",
    "        contentSize  = long(match.group(9)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We next read in the log files into an RDD\n",
    "\n",
    "log_files = \"/databricks-datasets/sample_logs\"\n",
    "raw_log_files = sc.textFile(log_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_log_files.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Let's go ahead and perform a transformation by parsing the log files using the customized function we have written\n",
    "\n",
    "parsed_log_files = raw_log_files.map(parse_apache_log_line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note again that no computation has been started, we've only created a logical execution plan that has not been realized yet. In Apache Spark terminology we're specifying a transformation. By specifying transformations and not performing them right away, this allows Spark to do a lot of optimizations under the hood most relevantly, pipelining. Pipelining means that Spark will perform as much computation as it can in memory and in one stage as opposed to spilling to disk after each step.\n",
    "Now that we've set up the transformation for parsing this raw text file, we want to make it available in a more structured format. We'll do this by creating a DataFrame (using toDF()) and then a table (using registerTempTable()). This will give us the added advantage of working with the same dataset in multiple languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parsed_log_files.toDF().registerTempTable(\"log_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%sql select * from log_data limit 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"screenshots/temp22.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "name": "CustomizedFunctions_PySpark",
  "notebookId": 3532702525773748
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
